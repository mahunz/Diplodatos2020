{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h2>Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación</h2>\n",
    "<h3>Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones 2020</h3>\n",
    "<h3>Predicción de la Calidad de Servicio</h3>\n",
    "<h3>Introducción al Aprendizaje Automático</h3>\n",
    "</center>\n",
    "</left>\n",
    "<h4>Mentor: Martín Hunziker</h4>\n",
    "\n",
    "[Link Mentoria](https://sites.google.com/view/mentorias2020-diplodatos/ciencia-de-datos-aplicada-en-la-distribuci%C3%B3n-de-energ%C3%ADa-el%C3%A9ctrica?authuser=0).\n",
    "\n",
    "</left>\n",
    "</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "En la siguiente notebook, se presentará la consigna a seguir para el tercer práctico de la materia Introducción al Aprendizaje Automático y la segunda parte de la creación de variables. \n",
    "\n",
    "Como referencia para el análisis geográfico utilizaremos la notebook 2_0_Intro_Variables_georefrenciadas de Ramiro Caro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de las librerías necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "mpl.get_cachedir()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPoint, MultiLineString\n",
    "import contextily as ctx\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import sklearn as skl\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, Ridge\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, classification_report, roc_curve, auc\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "np.random.seed(0)  # Para mayor determinismo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigna para la creación de Features\n",
    "\n",
    "### I. Features Geográficas\n",
    "\n",
    "Continuando con lo iniciado en los análisis previos, vamos a trabajar con la generación de Features a partir de grafos. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consigna para Introducción al Aprendizaje Automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Preprocesamiento\n",
    "\n",
    "#### 1. Obtención del Dataset\n",
    "\n",
    "Trabajaremos con el Dataset aumentado con las variables que obtuvimos en el práctico 2 y en la creación de Features del presente práctico..\n",
    "\n",
    "\n",
    "#### 2. Normalización de Atributos\n",
    "\n",
    "Es posible que sea necesario normalizar las features de nuestro dataset, dado que muchos de los algoritmos de clasificación supervisada lo requieren. ¿En qué casos tendrá que implementarse normalización?\n",
    "\n",
    "Aplicar al datasets la normalización de atributos que consideren adecuada.\n",
    "\n",
    "#### 3. Mezca Aleatória y División en Train/Test\n",
    "\n",
    "Finalmente, están en condiciones de **dividir el dataset en Train y Test**, utilizando para este último conjunto un 20% de los datos disponibles. Previo a esta división, es recomendable que mezclen los datos aleatoriamente.\n",
    "De este modo, deberán obtener cuatro conjuntos de datos, para cada uno de los datasets: ```X_train```, ```X_test```, ```y_train``` y ```y_test```.\n",
    "\n",
    "### III. Aplicación de Modelos de Clasificación\n",
    "\n",
    "Una vez finalizada la etapa de preprocesamiento, se propone implementar diferentes modelos de clasificación, utilizando la librería Scikit-Learn:\n",
    "\n",
    "1. Perceptron. Utilizar el método Stochastic Gradient Descent.\n",
    "2. K Nearest Neighbors ó K Vecinos Más Cercanos.\n",
    "3. Regresión Logística. Utilizar el método Stochastic Gradient Descent.\n",
    "\n",
    "De estos tres tipos de modelos, cuál creen que es el más adecuado para nuestro caso de aplicación?\n",
    "\n",
    "**Elegir el modelo que consideren que mejor aplica a nuestro problema.** Para ello, recuerden que los pasos a seguir en la selección pueden esquematizarse como sigue:\n",
    "\n",
    "#### 1. Descripción de la Hipótesis\n",
    "\n",
    "¿Cuál es nuestro problema? ¿Cómo se caracteriza? ¿Cuál es la hipótesis?\n",
    "\n",
    "#### 2. Selección de Regularizador\n",
    "\n",
    " ¿Utilizarán algún regularizador?¿Cuál?\n",
    "\n",
    "#### 3. Selección de Función de Costo\n",
    "\n",
    "¿Cuál será la función de costo utilizada?\n",
    "\n",
    "#### 4. Justificación de las Selecciones\n",
    "\n",
    "¿Por qué eligieron el modelo, el regularizador y la función de costo previas?\n",
    "\n",
    "Finalmente, para el modelo selecionado:\n",
    "\n",
    "- Utilizar el método *Grid Search*, o de búsqueda exahustiva, con *cross-validation* para profundizar en la búsqueda y selección de hiperparámetros.\n",
    "- Calcular métricas sobre el conjunto de entrenamiento y de evaluación para los mejores parámetros obtenidos:\n",
    "    + Accuracy o exactitud\n",
    "    + Reporte de clasificación\n",
    "    + Confusion matrix o matriz de confusión (graficar como heatmap)\n",
    "    + Curva ROC y área bajo la curva (AUC).\n",
    "---\n",
    "\n",
    "Recuerden que la ciencia de datos es un **proceso circular, continuo y no lineal**. Es decir, si los datos requieren de mayor procesamiento para satisfacer las necesidades de algoritmos de ML (cualesquiera de ellos), vamos a volver a la etapa inicial para, por ejemplo, crear nuevas features, tomar decisiones diferentes sobre valores faltantes o valores atípicos (outliers), descartar features, entre otras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Análisis  Contenido (Cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Features geográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pa-bOWcEhua5"
   },
   "source": [
    "Cargue el Dataset con las variables generadas en el práctico 2.\n",
    "\n",
    "#### Generación de grafos y calculo de rutas\n",
    "\n",
    "1 - Usando las técnicas mostradas en el notebook sobre procesamiento de información georeferenciada, construya un grafo basado en la capa de línea de distribición de media tensión:\n",
    "  - Utilize los puntos de conexión PCON_1 y PCON_2 como nodos, y los segmentos como edges.\n",
    "  - Adicione COMP y COD_ID como atributos de edge (aristas)\n",
    "  - Asocie cada transformador untrd con un nodo\n",
    "  - Asocie cada punto de conexión los circuitos de media tension (CTMT) con la subestación, a un nodo.\n",
    "\n",
    "2 - Seleccione al menos 5 métricas de grafo, calcule sus valores para los nodos asociados a cada fila y adicionelos como features al dataset.\n",
    "\n",
    "3 - Calcule la distancia de cada untrd a su correspondiente punto de conexión con la subestación, ponderando por COMP (longitud de segmento), utilize el parametro \"method\" para calcular diferentes tipos de distancias.\n",
    "Adicione los resultados como variables del dataset.\n",
    "\n",
    "4 - Calcule la ruta (secuencia de nodos) hasta la conexión con la subestación, y en base a esta calcule:\n",
    " - Resistencia eléctica total de cada conexión\n",
    " - Reactancia eléctrica de cada conexión.\n",
    " [Opcional]\n",
    " - Modulo y ángulo de la Impedancia.\n",
    " - Corriente Nonimal media a lo largo de la ruta\n",
    " - Corriente Maxima media del conductor a lo largo la ruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preprocesamiento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Normalización de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar al dataset la normalización de atributos que consideren adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pueden utilizar los siguientes métodos, por ejemplo:\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "standard_scaler = preprocessing.StandardScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mezca Aleatória y División en Train/Test\n",
    "\n",
    "Primeramente, deberán mezclar los datos aleatoriamente. Luego, para dividir en Train/Test el dataset, aplicar el split utilizando un 20% de datos para este último.\n",
    "\n",
    "En este punto, deberán obtener cuatro conjuntos de datos, para ambos datasets: ```X_train```, ```X_test```, ```y_train``` y ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para dividir el dataset, utilizar el siguiente módulo:\n",
    "\n",
    "_ds_shuff = shuffle(_ds)\n",
    "\n",
    "# Y luego el módulo:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Notar que X e y son np.arrays. Además, pueden usar el parámetro que incluye train_test_split para mezclar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Aplicación de Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perceptron\n",
    "A continuación se aplicará un clasificador Perceptrón.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En principio, pueden utilizar el módulo que sigue, con los parámetros por defecto y los que definan a continuación:\n",
    "penalty =   \n",
    "alpha = \n",
    "max_iter =\n",
    "tol = \n",
    "\n",
    "model = Perceptron(penalty = penalty, alpha = alpha, fit_intercept=True, max_iter = max_iter, tol = tol, shuffle=True, random_state=0, class_weight=None, warm_start=False)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo sería utilizando el método Stochastic Gradient Descent?\n",
    "\n",
    "_Hint:_ https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n",
    "y_pred_train =   # Obtenemos las predicciones para conjunto de entrenamiento\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test =   # Obtenemos las predicciones para conjunto de validación\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se aplicará un clasificador K-NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors =  # TODO: Cantidad de vecinos a tener en cuenta\n",
    "metric =  # TODO: Medida de distancia. Algunas opciones: cosine, euclidean, manhattan.\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n",
    "y_pred_train = model.predict(X_train)  # Obtenemos las predicciones para conjunto de entrenamiento\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de entrenamiento: %.2f\" % accuracy_train)\n",
    "\n",
    "y_pred_test = model.predict(X_test)  # Obtenemos las predicciones para conjunto de validación\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Exactitud del algoritmo para conjunto de validación: %.2f\" % accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty =   # TODO: Tipo de regularización: l1 (valor absoluto), l2 (cuadrados).\n",
    "alpha =   # TODO: Parámetro de regularización. También denominado como parámetro `lambda`. Debe ser mayor que 0.\n",
    "max_iter =   # TODO: Cantidad máxima de iteraciones del algoritmo.\n",
    "tol =   # TODO: Precisión del algoritmo (error mínimo entre una iteración y la siguiente).\n",
    "\n",
    "model = LogisticRegression(penalty=penalty, C=1./alpha, max_iter=max_iter, tol=tol)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "¿Cómo sería utilizando el método Stochastic Gradient Descent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se calcula la exactitud para ambos conjuntos, train y test:\n",
    "\n",
    "print('Exactitud para entrenamiento: %.2f' %  accuracy_score(y_train, model.predict(X_train)))\n",
    "print('Exactitud para validación: %.2f' % accuracy_score(y_val, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Selección del Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Selección y Descripción de Hipótesis\n",
    "\n",
    "Describir el problema y la hipótesis del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Selección de Regularizador\n",
    "\n",
    " ¿Utilizarán algún regularizador?¿Cuál?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Selección de Función de Costo\n",
    "\n",
    "¿Cuál será la función de costo utilizada?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Justificación de las Selecciones\n",
    "\n",
    "A continuación, se justifican las elecciones previas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Selección de Parámetros y Métricas Sobre el Conjunto de Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la selección de hiperparámetros, pueden utilizar GridSearch. Además, deben calcular las métricas solicitadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para la búsqueda de los mejores parámetros, por ejemplo de logistic regression, pueden usar:\n",
    "\n",
    "exploring_params = {\n",
    "        'C': [0.5, 1, 2, 5, 10, 20, 100, 200], # Inversa del coeficiente de regularización\n",
    "        'max_iter': [1000, 5000, 10000],  # Cantidad de iteraciones\n",
    "        'tol': [0.005, 0.002, 0.001, 0.0001]  # Precisión del algoritmo\n",
    "    }\n",
    "\n",
    "m = LogisticRegression(penalty, loss)\n",
    "n_cross_val =   # Seleccionar folds\n",
    "scoring = 'roc_auc'\n",
    "model = GridSearchCV(m, exploring_params, cv=n_cross_val, scoring=scoring)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejor conjunto de parámetros:\")\n",
    "print(model.best_params_, end=\"\\n\\n\")\n",
    "print()\n",
    "print(\"Puntajes de la grilla:\", end=\"\\n\\n\")\n",
    "means = model.cv_results_['mean_test_score']\n",
    "stds = model.cv_results_['std_test_score'])\n",
    "print()\n",
    "print(\"Reporte de clasificación para el mejor clasificador (sobre conjunto de evaluación):\", end=\"\\n\\n\")\n",
    "y_true, y_pred = y_test, model.predict(X_test)\n",
    "print(classification_report(y_true, y_pred), end=\"\\n\\n\")\n",
    "\n",
    "print(\"================================================\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las métricas solicitadas son: accuracy_score, confusion_matrix, classification_report, roc_curve, auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
