{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "tutorial mitigar bias en word embeddings .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yiranamejia/Diplodatos2020/blob/https%2Fcolab.research.google.com%2Fdrive%2F1GWE8PVGwvpo1ZxRket7xszJy3Q9DQABn%23scrollTo%3DyoJDoeIzTrqz/tutorial_mitigar_bias_en_word_embeddings_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TylXHnmHHPSm"
      },
      "source": [
        "# **Diagnóstico y mitigación de sesgo de género en embeddings de palabras**\n",
        "\n",
        "## basado en el workshop https://learn.responsibly.ai/word-embedding\n",
        "\n",
        "y sobre el toolkit [`responsibly`](https://docs.responsibly.ai/) - para auditar y mitigar sesgo y obtener equidad en los sistemas de aprendizaje automático.\n",
        "\n",
        "# Descargos\n",
        "\n",
        "En este ejemplo nos enfocamos en sesgo de género simplificándolo a un fenómeno binario, pero entendemos que se trata de una sobresimplificación, una primera aproximación a la familia de soluciones de mitigación que requiere de una mayor complejidad para tratar los fenómenos de sesgo como construcciones sociales.\n",
        "\n",
        "Este material es un ejercicio puntual, no una perspectiva completa sobre sesgo en aprendizaje automático, equidad o inteligencia artificial responsable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4nFGQ75HPSn"
      },
      "source": [
        "# Configuración"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHn7LJEFHPSn"
      },
      "source": [
        "## Instalar `responsibly`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PNDdTweHPSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a895bb-2de4-47a4-d99b-2dbb9aa6348a"
      },
      "source": [
        "!pip install --user responsibly"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: responsibly in /root/.local/lib/python3.6/site-packages (0.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19 in /usr/local/lib/python3.6/dist-packages (from responsibly) (0.22.2.post1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from responsibly) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from responsibly) (1.1.5)\n",
            "Requirement already satisfied: seaborn>=0.9 in /usr/local/lib/python3.6/dist-packages (from responsibly) (0.11.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.6/dist-packages (from responsibly) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.24 in /usr/local/lib/python3.6/dist-packages (from responsibly) (4.41.1)\n",
            "Requirement already satisfied: matplotlib<3,>=2.2 in /root/.local/lib/python3.6/site-packages (from responsibly) (2.2.5)\n",
            "Requirement already satisfied: mlxtend<0.17,>=0.13 in /usr/local/lib/python3.6/dist-packages (from responsibly) (0.14.0)\n",
            "Requirement already satisfied: gensim>=3.7 in /root/.local/lib/python3.6/site-packages (from responsibly) (3.8.3)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.6/dist-packages (from responsibly) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from responsibly) (1.18.5)\n",
            "Requirement already satisfied: tabulate>=0.8 in /usr/local/lib/python3.6/dist-packages (from responsibly) (0.8.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19->responsibly) (0.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->responsibly) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->responsibly) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3,>=2.2->responsibly) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3,>=2.2->responsibly) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib<3,>=2.2->responsibly) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend<0.17,>=0.13->responsibly) (50.3.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.7->responsibly) (4.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haXHrM7eHPSn"
      },
      "source": [
        "## Validar la instalación de `responsibly`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx-k9YV9HPSn"
      },
      "source": [
        "import responsibly\n",
        "\n",
        "# deberían obtener '0.1.2'\n",
        "responsibly.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwbS87m7HPSn"
      },
      "source": [
        "---\n",
        "\n",
        "Si están trabajando en Colab, es normal que después de la instalación tengan el error **`ModuleNotFoundError: No module named 'responsibly'`**.\n",
        "<br/> <br/>\n",
        "Reinicien el Kernel/Runtime (usen el menú de arriba o el botón en la notebook), salteen la celda de instalación (`!pip install --user responsibly`) y ejecuten la celda previa de vuelta. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aN-ZeM5HPSo"
      },
      "source": [
        "# Jugar con el embedding de Word2Vec\n",
        "\n",
        "Con el paquete [`responsibly`](http://docs.responsibly.ai) viene la función [`responsibly.we.load_w2v_small`]() que devuelve un objeto [`KeyedVectors`](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors) de [`gensim`](https://radimrehurek.com/gensim/). Este modelo fue entrenado con Google News - 100B tokens, vocabulario de 3 millones, vectores de 300 dimensiones, sólo nos quedamos con el vocabulario en minúscula.\n",
        "\n",
        "Para más información: [Word2Vec](https://code.google.com/archive/p/word2vec/) - \n",
        "\n",
        "## Propiedades Básicas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MbuyNyFHPSo"
      },
      "source": [
        "# ignorar warnings\n",
        "# en general no queremos hacerlo pero ahora nos queremos enfocar en otra cosa\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2yChPuMHPSo"
      },
      "source": [
        "from responsibly.we import load_w2v_small\n",
        "\n",
        "w2v_small = load_w2v_small()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWa0pheRHPSo"
      },
      "source": [
        "# tamanio del vocabulario\n",
        "\n",
        "len(w2v_small.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tilWEJIDHPSo"
      },
      "source": [
        "# obtener el vector de la palabra \"home\"\n",
        "\n",
        "print('home =', w2v_small['home'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6DWNcVsHPSo"
      },
      "source": [
        "# la dimensión del embedding de la palabra, en este caso, es 300\n",
        "\n",
        "len(w2v_small['home'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5i6Nd3EHPSo"
      },
      "source": [
        "# todas las palabras están normalizadas (=tienen una norma igual a uno como vectores)\n",
        "\n",
        "from numpy.linalg import norm\n",
        "\n",
        "norm(w2v_small['home'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfkBhxYuHPSo"
      },
      "source": [
        "# asegurémonos que todos los vectores están normalizados\n",
        "\n",
        "from numpy.testing import assert_almost_equal\n",
        "\n",
        "length_vectors = norm(w2v_small.vectors, axis=1)\n",
        "\n",
        "assert_almost_equal(actual=length_vectors,\n",
        "                    desired=1,\n",
        "                    decimal=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhCem0fsHPSo"
      },
      "source": [
        "## Medir la similitud entre palabras\n",
        "\n",
        "Usaremos el [coseno](https://es.wikipedia.org/wiki/Similitud_coseno) como medida de similitud (o distancia) entre palabras.\n",
        "- Mide el coseno del ángulo entre dos vectores.\n",
        "- Rango entre 1 (vectores idénticos) y -1 (vectores opuestos).\n",
        "- En Python, para vectores normalizados (arrays de Numpy), usar el operador `@`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-7nbBcsHPSo"
      },
      "source": [
        "w2v_small['cat'] @ w2v_small['cat']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwBriungHPSo"
      },
      "source": [
        "w2v_small['cat'] @ w2v_small['cats']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU2_chyBHPSo"
      },
      "source": [
        "from math import acos, degrees\n",
        "\n",
        "degrees(acos(w2v_small['cat'] @ w2v_small['cats']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmmYgVTyHPSo"
      },
      "source": [
        "w2v_small['cat'] @ w2v_small['dog']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZ4QGX3QHPSo"
      },
      "source": [
        "degrees(acos(w2v_small['cat'] @ w2v_small['dog']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vAGtxd4HPSo"
      },
      "source": [
        "w2v_small['cat'] @ w2v_small['cow']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO9Ymx-CHPSo"
      },
      "source": [
        "degrees(acos(w2v_small['cat'] @ w2v_small['cow']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EXNejAxHPSo"
      },
      "source": [
        "w2v_small['cat'] @ w2v_small['graduated']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLR5i4QIHPSo"
      },
      "source": [
        "degrees(acos(w2v_small['cat'] @ w2v_small['graduated']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqHoqJnQHPSo"
      },
      "source": [
        "## Visualización del Word Embedding usando T-SNE \n",
        "\n",
        "<small>fuente: [Google's Seedbank](https://research.google.com/seedbank/seed/pretrained_word_embeddings)</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSaC_nyXHPSo"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pylab as plt\n",
        "\n",
        "# take the most common words in the corpus between 200 and 600\n",
        "words = [word for word in w2v_small.index2word[200:600]]\n",
        "\n",
        "# convert the words to vectors\n",
        "embeddings = [w2v_small[word] for word in words]\n",
        "\n",
        "# perform T-SNE\n",
        "words_embedded = TSNE(n_components=2).fit_transform(embeddings)\n",
        "\n",
        "# ... and visualize!\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, label in enumerate(words):\n",
        "    x, y = words_embedded[i, :]\n",
        "    plt.scatter(x, y)\n",
        "    plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
        "                 ha='right', va='bottom', size=11)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2laNOxSAHPSo"
      },
      "source": [
        "### Extra: [Tensorflow Embedding Projector](http://projector.tensorflow.org)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0orOF4LHPSo"
      },
      "source": [
        "## Palabras más semejantes\n",
        "\n",
        "Cuáles son las palabras más semejantes a una determinada palabra?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr8o2o_THPSo"
      },
      "source": [
        "w2v_small.most_similar('cat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQLpS8IlHPSp"
      },
      "source": [
        "### EXTRA: Cuál es la palabra que desentona?\n",
        "\n",
        "Dada una lista de palabras, cuál desentona? Es decir, cuál es la que está más lejos de la media de palabras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr3hQvF0HPSp"
      },
      "source": [
        "w2v_small.doesnt_match('breakfast cereal dinner lunch'.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT_RDdLFHPSp"
      },
      "source": [
        "## Suma de palabras\n",
        "\n",
        "![](https://github.com/ResponsiblyAI/word-embedding/blob/master/images/vector-addition.png?raw=1)\n",
        "\n",
        "<small>fuente: [Wikipedia](https://commons.wikimedia.org/wiki/File:Vector_add_scale.svg)</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "safkv9RCHPSp"
      },
      "source": [
        "# nature + science = ?\n",
        "\n",
        "w2v_small.most_similar(positive=['nature', 'science'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgRMunP4HPSp"
      },
      "source": [
        "## Analogía de vectores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo1smCA6HPSp"
      },
      "source": [
        "![](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "<small>fuente: [Documentación de Tensorflow](https://www.tensorflow.org/tutorials/representation/word2vec)</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMEgOEhgHPSp"
      },
      "source": [
        "# man:king :: woman:?\n",
        "# king - man + woman = ?\n",
        "\n",
        "w2v_small.most_similar(positive=['king', 'woman'],\n",
        "                       negative=['man'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DudawGFKHPSp"
      },
      "source": [
        "w2v_small.most_similar(positive=['big', 'smaller'],\n",
        "                       negative=['small'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja00u1VDHPSp"
      },
      "source": [
        "## La dirección de un embedding puede verse como una relación\n",
        "\n",
        "# $\\overrightarrow{she} - \\overrightarrow{he}$\n",
        "# $\\overrightarrow{smaller} - \\overrightarrow{small}$\n",
        "# $\\overrightarrow{Spain} - \\overrightarrow{Madrid}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9OBIpv2HPSp"
      },
      "source": [
        "# Diagnosticamos sesgo de género en embeddings\n",
        "\n",
        "Bolukbasi Tolga, Kai-Wei Chang, James Y. Zou, Venkatesh Saligrama, and Adam T. Kalai. [Man is to computer programmer as woman is to homemaker? debiasing word embeddings](https://arxiv.org/abs/1607.06520). NIPS 2016.\n",
        "\n",
        "¿Cómo afecta el sesgo de género en embeddings en el contexto de aplicaciones downstream?\n",
        "\n",
        "![](https://github.com/ResponsiblyAI/word-embedding/blob/master/images/examples-gender-bias-nlp.png?raw=1)\n",
        "\n",
        "<small>fuente: Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... & Wang, W. Y. (2019). [Mitigating Gender Bias in Natural Language Processing: Literature Review](https://arxiv.org/pdf/1906.08976.pdf). arXiv preprint arXiv:1906.08976.</small>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPSnOYQIHPSp"
      },
      "source": [
        "## Probemos algunas propiedades con expresiones que sabemos que están fuertemente marcadas por el género\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEDgqRlFHPSp"
      },
      "source": [
        "# she:sister :: he:?\n",
        "# sister - she + he = ?\n",
        "\n",
        "w2v_small.most_similar(positive=['sister', 'he'],\n",
        "                       negative=['she'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLRbogzEHPSp"
      },
      "source": [
        "```\n",
        "queen-king\n",
        "waitress-waiter\n",
        "sister-brother\n",
        "mother-father\n",
        "ovarian_cancer-prostate_cancer\n",
        "convent-monastery\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlErzh68HPSp"
      },
      "source": [
        "w2v_small.most_similar(positive=['nurse', 'he'],\n",
        "                       negative=['she'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K74Iz-14HPSp"
      },
      "source": [
        "```\n",
        "sewing-carpentry\n",
        "nurse-doctor\n",
        "blond-burly\n",
        "giggle-chuckle\n",
        "sassy-snappy\n",
        "volleyball-football\n",
        "register_nurse-physician\n",
        "interior_designer-architect\n",
        "feminism-conservatism\n",
        "vocalist-guitarist\n",
        "diva-superstar\n",
        "cupcakes-pizzas\n",
        "housewife-shopkeeper\n",
        "softball-baseball\n",
        "cosmetics-pharmaceuticals\n",
        "petite-lanky\n",
        "charming-affable\n",
        "hairdresser-barber\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvfPt1T-HPSp"
      },
      "source": [
        "Parece que el método de generar analogías no es la forma más adecuada de observar sesgo en los embeddings, por la paradoja del observador: introduce sesgo, fuerza la producción de estereotipos de género!\n",
        "\n",
        "Nissim, M., van Noord, R., van der Goot, R. (2019). [Fair is Better than Sensational: Man is to Doctor as Woman is to Doctor](https://arxiv.org/abs/1905.09866).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2F_l7OeHPSp"
      },
      "source": [
        "## Qué sí nos da la analogía? La dirección del género!\n",
        "\n",
        "# $\\overrightarrow{she} - \\overrightarrow{he}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGjRzqqbHPSp"
      },
      "source": [
        "gender_direction = w2v_small['she'] - w2v_small['he']\n",
        "\n",
        "gender_direction /= norm(gender_direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw3Xed-kHPSp"
      },
      "source": [
        "gender_direction @ w2v_small['architect']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4Ip0nTKHPSp"
      },
      "source": [
        "gender_direction @ w2v_small['interior_designer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RuU3hbEHPSp"
      },
      "source": [
        "Con todos los recaudos de saber que estamos sobresimplificando el fenómeno, podemos ver que la palabra *architect* aparece en más contextos con *he* que con *she*, y viceversa para *interior designer*.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8uZ6qLQHPSp"
      },
      "source": [
        "Basándonos en esta propiedad, podemos calcular la dirección del género usano varios pares de palabras que sabemos que están fuertemente marcadas para género.:\n",
        "\n",
        "- woman - man\n",
        "- girl - boy\n",
        "- she - he\n",
        "- mother - father\n",
        "- daughter - son\n",
        "- gal - guy\n",
        "- female - male\n",
        "- her - his\n",
        "- herself - himself\n",
        "- Mary - John"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYl3D_ATHPSp"
      },
      "source": [
        "## Prueben con algunas palabras\n",
        "Reflexión: ¿están haciendo análisis exploratorio o evaluación sistemática?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCGf0oHBHPSp"
      },
      "source": [
        "gender_direction @ w2v_small['word']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshpmrI9HPSp"
      },
      "source": [
        "Proyecciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhfOAPsTHPSp"
      },
      "source": [
        "from responsibly.we import GenderBiasWE\n",
        "\n",
        "w2v_small_gender_bias = GenderBiasWE(w2v_small, only_lower=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7OjvenQHPSp"
      },
      "source": [
        "w2v_small_gender_bias.positive_end, w2v_small_gender_bias.negative_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBSW9vuSHPSq"
      },
      "source": [
        "# dirección del género\n",
        "w2v_small_gender_bias.direction[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WxA6tfvHPSq"
      },
      "source": [
        "from responsibly.we.data import BOLUKBASI_DATA\n",
        "\n",
        "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lSOGF5BHPSq"
      },
      "source": [
        "neutral_profession_names[:8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzjYnRJ8HPSq"
      },
      "source": [
        "Nota: `actor` está en la lista de nombres de profesión neutros, y no`actress` porque parece que el uso de la palabra ha cambiado con el tiempo y ahora es más neutro, en comparación por ejemplo con  waiter-waitress (ver [Wikipedia - The term Actress](https://en.wikipedia.org/wiki/Actor#The_term_actress))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI93U0JlHPSq"
      },
      "source": [
        "len(neutral_profession_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JS7UZjyHPSs"
      },
      "source": [
        "# the same of using the @ operator on the bias direction\n",
        "\n",
        "w2v_small_gender_bias.project_on_direction(neutral_profession_names[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDPo28WyHPSs"
      },
      "source": [
        "Visualicemos las proyecciones de las profesiones (neutras y específicas) en la dirección del género."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "zEnirv55HPSt"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "f, ax = plt.subplots(1, figsize=(10, 10))\n",
        "\n",
        "w2v_small_gender_bias.plot_projection_scores(n_extreme=20, ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z_VCT-eHPSt"
      },
      "source": [
        "EXTRA: Demo - Visualizando sesgo de género con [Nubes de palabras](http://wordbias.umiacs.umd.edu/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtcH2sX_HPSt"
      },
      "source": [
        "Las proyecciones en la dirección de género de las palabras de profesiones se corresponden con datos de ocupación desglosados por género, según se puede ver en el porcentaje de mujeres en diversas profesiones según la encuesta de población de 2017 del Labor Force Statistics: https://arxiv.org/abs/1804.06876"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZoIxVJ4HPSt"
      },
      "source": [
        "from operator import itemgetter  # 🛠️ For idiomatic sorting in Python\n",
        "\n",
        "from responsibly.we.data import OCCUPATION_FEMALE_PRECENTAGE\n",
        "\n",
        "sorted(OCCUPATION_FEMALE_PRECENTAGE.items(), key=itemgetter(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSrQIXF7HPSt"
      },
      "source": [
        "f, ax = plt.subplots(1, figsize=(10, 8))\n",
        "\n",
        "w2v_small_gender_bias.plot_factual_association(ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQw3uMaDHPSt"
      },
      "source": [
        "Garg, N., Schiebinger, L., Jurafsky, D., & Zou, J. (2018). [Word embeddings quantify 100 years of gender and ethnic stereotypes](https://www.pnas.org/content/pnas/115/16/E3635.full.pdf). Proceedings of the National Academy of Sciences, 115(16), E3635-E3644.\n",
        "\n",
        "![](https://github.com/ResponsiblyAI/word-embedding/blob/master/images/gender-bias-over-decades.png?raw=1)\n",
        "\n",
        "<small>Data: Google Books/Corpus of Historical American English (COHA)</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kve5HaIUHPSt"
      },
      "source": [
        "## Medición directa del sesgo\n",
        "\n",
        "1. Proyectamos cada uno de los nombres de profesión neutros en la dirección de género\n",
        "2. Calculamos el valor absoluto de cada proyección\n",
        "3. Lo promediamos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEN5PVPsHPSu"
      },
      "source": [
        "# función de alto nivel en responsibly\n",
        "\n",
        "w2v_small_gender_bias.calc_direct_bias()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgtF2eUqHPSu"
      },
      "source": [
        "# qué hace responsibly internamente:\n",
        "\n",
        "neutral_profession_projections = [w2v_small[word] @ w2v_small_gender_bias.direction\n",
        "                                  for word in neutral_profession_names]\n",
        "\n",
        "abs_neutral_profession_projections = [abs(proj) for proj in neutral_profession_projections]\n",
        "\n",
        "sum(abs_neutral_profession_projections) / len(abs_neutral_profession_projections)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZoPjpEeHPSu"
      },
      "source": [
        "**Atención** la medición directa de sesgo está haciendo asunciones fuertes sobre las palabras neutras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwoxH_RgHPSu"
      },
      "source": [
        "## 5.10 - [EXTRA] Medición indirecta del sesgo\n",
        "Semejanza por proyección en la misma \"dirección de género\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pamry1hmHPSv"
      },
      "source": [
        "w2v_small_gender_bias.generate_closest_words_indirect_bias('softball',\n",
        "                                                           'football')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlS0ydglHPSv"
      },
      "source": [
        "# Mitigar sesgo\n",
        "\n",
        "> We intentionally do not reference the resulting embeddings as \"debiased\" or free from all gender bias, and\n",
        "prefer the term \"mitigating bias\" rather that \"debiasing,\" to guard against the misconception that the resulting\n",
        "embeddings are entirely \"safe\" and need not be critically evaluated for bias in downstream tasks. <small>James-Sorenson, H., & Alvarez-Melis, D. (2019). [Probabilistic Bias Mitigation in Word Embeddings](https://arxiv.org/pdf/1910.14497.pdf). arXiv preprint arXiv:1910.14497.</small>\n",
        "\n",
        "\n",
        "## Neutralizar\n",
        "\n",
        "Si neutralizamos, vamos a eliminar la proyección de género de todas las palabras excepto las de género neutro, y después normalizamos.\n",
        "\n",
        "**Atención** un prerequisito fuerte es tener la lista de palabras fuertemente marcadas para género."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGGDnQHTHPSv"
      },
      "source": [
        "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='neutralize', inplace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVoPZ9qvHPSv"
      },
      "source": [
        "print('home:',\n",
        "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
        "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pum8xVStHPSv"
      },
      "source": [
        "print('man:',\n",
        "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
        "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiN3so-cHPSv"
      },
      "source": [
        "print('woman:',\n",
        "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
        "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSbuQ_0nHPSv"
      },
      "source": [
        "w2v_small_gender_debias.calc_direct_bias()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nviSfKo0HPSv"
      },
      "source": [
        "f, ax = plt.subplots(1, figsize=(10, 10))\n",
        "\n",
        "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytG7tYD6HPSv"
      },
      "source": [
        "f, ax = plt.subplots(1, figsize=(10, 8))\n",
        "\n",
        "w2v_small_gender_debias.plot_factual_association(ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D72h6h_-HPSv"
      },
      "source": [
        "## [EXTRA] Ecualizar\n",
        "\n",
        "Las palabras en la lista de palabras marcadas para género (como por ejemplo `man` y `woman`) pueden tener una proyección diferente en la dirección de género. Eso puede resultar en una similitud diferente a palabras neutras, como `kitchen`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NpdcrJVHPSv"
      },
      "source": [
        "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jSEfiZPHPSv"
      },
      "source": [
        "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEwqwZXoHPSv"
      },
      "source": [
        "BOLUKBASI_DATA['gender']['equalize_pairs'][:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vELxkXLHPSv"
      },
      "source": [
        "## Eliminación de sesgo dura: Neutralizar y Ecualizar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm9XmdjiHPSw"
      },
      "source": [
        "w2v_small_gender_debias = w2v_small_gender_bias.debias(method='hard', inplace=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScM9OhHwHPSw"
      },
      "source": [
        "print('home:',\n",
        "      'before =', w2v_small_gender_bias.model['home'] @ w2v_small_gender_bias.direction,\n",
        "      'after = ', w2v_small_gender_debias.model['home'] @ w2v_small_gender_debias.direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTLS5X6WHPSx"
      },
      "source": [
        "print('man:',\n",
        "      'before =', w2v_small_gender_bias.model['man'] @ w2v_small_gender_bias.direction,\n",
        "      'after = ', w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YxQoXzOHPSx"
      },
      "source": [
        "print('woman:',\n",
        "      'before =', w2v_small_gender_bias.model['woman'] @ w2v_small_gender_bias.direction,\n",
        "      'after = ', w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.direction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv_ioBbkHPSx"
      },
      "source": [
        "w2v_small_gender_debias.calc_direct_bias()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7TlMzRUHPSx"
      },
      "source": [
        "w2v_small_gender_debias.model['man'] @ w2v_small_gender_debias.model['kitchen']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMtftVhxHPSx"
      },
      "source": [
        "w2v_small_gender_debias.model['woman'] @ w2v_small_gender_debias.model['kitchen']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLwPjbgIHPSx"
      },
      "source": [
        "f, ax = plt.subplots(1, figsize=(10, 10))\n",
        "\n",
        "w2v_small_gender_debias.plot_projection_scores(n_extreme=20, ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYfDqt5aHPSx"
      },
      "source": [
        "Después de mitigar el sesgo, el rendimiento del embedding resultante en benchmarks estándares no se ve fuertemente afectado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3f3WQhgHPSx"
      },
      "source": [
        "w2v_small_gender_bias.evaluate_word_embedding()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB_b9z16HPSx"
      },
      "source": [
        "w2v_small_gender_debias.evaluate_word_embedding()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL5krvKnHPS1"
      },
      "source": [
        "# Explorar otros tipos de sesgo en word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblQa1vKHPS1"
      },
      "source": [
        "### Sesgo racial\n",
        "\n",
        "Usaremos la clase [`responsibly.we.BiasWordEmbedding`](http://docs.responsibly.ai/word-embedding-bias.html#ethically.we.bias.BiasWordEmbedding). `GenderBiasWE` es una subclase de `BiasWordEmbedding`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXOGwjCgHPS1"
      },
      "source": [
        "from responsibly.we import BiasWordEmbedding\n",
        "\n",
        "w2v_small_racial_bias = BiasWordEmbedding(w2v_small, only_lower=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_m-sCQPHPS2"
      },
      "source": [
        "Identificar la dirección racial usando el método `sum`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NgemvQPHPS2"
      },
      "source": [
        "white_common_names = ['Emily', 'Anne', 'Jill', 'Allison', 'Laurie', 'Sarah', 'Meredith', 'Carrie',\n",
        "                      'Kristen', 'Todd', 'Neil', 'Geoffrey', 'Brett', 'Brendan', 'Greg', 'Matthew',\n",
        "                      'Jay', 'Brad']\n",
        "\n",
        "black_common_names = ['Aisha', 'Keisha', 'Tamika', 'Lakisha', 'Tanisha', 'Latoya', 'Kenya', 'Latonya',\n",
        "                      'Ebony', 'Rasheed', 'Tremayne', 'Kareem', 'Darnell', 'Tyrone', 'Hakim', 'Jamal',\n",
        "                      'Leroy', 'Jermaine']\n",
        "\n",
        "w2v_small_racial_bias._identify_direction('Whites', 'Blacks',\n",
        "                                          definitional=(white_common_names, black_common_names),\n",
        "                                          method='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDuYsHv7HPS2"
      },
      "source": [
        "Usar los nombres de profesión neutros para medir el sesgo racial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQxxl7LIHPS2"
      },
      "source": [
        "neutral_profession_names = BOLUKBASI_DATA['gender']['neutral_profession_names']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSVyfTklHPS2"
      },
      "source": [
        "neutral_profession_names[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGMT4tNfHPS2"
      },
      "source": [
        "f, ax = plt.subplots(1, figsize=(10, 10))\n",
        "\n",
        "w2v_small_racial_bias.plot_projection_scores(neutral_profession_names, n_extreme=20, ax=ax);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPn1vjxZHPS2"
      },
      "source": [
        "Calcular la medida directa de sesgo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2zrnHd6HPS2"
      },
      "source": [
        "# Your Code Here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJFRjlJ1HPS2"
      },
      "source": [
        "Sigan explorando el sesgo racial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toREKl5YHPS2"
      },
      "source": [
        "# Your Code Here..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuF8ZOFWHPS3"
      },
      "source": [
        "# Recursos\n",
        "\n",
        "## [Doing Data Science Responsibly - Resources](https://handbook.responsibly.ai/appendices/resources.html)\n",
        "\n",
        "In particular:\n",
        "\n",
        "- CVPR 2020 - [FATE Tutorial](https://youtu.be/-xGvcDzvi7Q) [Video]\n",
        "\n",
        "- fast.ai - [Algorithmic Bias (NLP video 16)](https://youtu.be/pThqge9QDn8) [Video]\n",
        "\n",
        "-  Solon Barocas, Moritz Hardt, Arvind Narayanan - [Fairness and machine learning - Limitations and Opportunities](https://fairmlbook.org/) [Textbook]\n",
        "\n",
        "\n",
        "\n",
        "## Non-Technical Overview with More Downstream Application Examples\n",
        "- [Google - Text Embedding Models Contain Bias. Here's Why That Matters.](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html)\n",
        "- [Kai-Wei Chang (UCLA) - What It Takes to Control Societal Bias in Natural Language Processing](https://www.youtube.com/watch?v=RgcXD_1Cu18)\n",
        "- Sun, T., Gaut, A., Tang, S., Huang, Y., ElSherief, M., Zhao, J., ... & Wang, W. Y. (2019). [Mitigating Gender Bias in Natural Language Processing: Literature Review](https://arxiv.org/pdf/1906.08976.pdf). arXiv preprint arXiv:1906.08976.\n",
        "\n",
        "## Additional Related Work\n",
        "\n",
        "- **Understanding Bias**\n",
        "    - Ethayarajh, K., Duvenaud, D., & Hirst, G. (2019, July). [Understanding Undesirable Word Embedding Associations](https://arxiv.org/pdf/1908.06361.pdf). In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 1696-1705). - **Including critical analysis of the current metrics and debiasing methods (quite technical)**\n",
        "\n",
        "  - Brunet, M. E., Alkalay-Houlihan, C., Anderson, A., & Zemel, R. (2019, May). [Understanding the Origins of Bias in Word Embeddings](https://arxiv.org/pdf/1810.03611.pdf). In International Conference on Machine Learning (pp. 803-811).\n",
        "\n",
        "\n",
        "- **Discovering Biases**\n",
        "  - Swinger, N., De-Arteaga, M., Heffernan IV, N. T., Leiserson, M. D., & Kalai, A. T. (2019, January). [What are the biases in my word embedding?](https://arxiv.org/pdf/1812.08769.pdf). In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 305-311). ACM.\n",
        "    Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories\n",
        "  \n",
        "  - Chaloner, K., & Maldonado, A. (2019, August). [Measuring Gender Bias in Word Embeddings across Domains and Discovering New Gender Bias Word Categories](https://www.aclweb.org/anthology/W19-3804). In Proceedings of the First Workshop on Gender Bias in Natural Language Processing (pp. 25-32).\n",
        "\n",
        "\n",
        "- **Fairness in Classification**\n",
        "  - Prost, F., Thain, N., & Bolukbasi, T. (2019, August). [Debiasing Embeddings for Reduced Gender Bias in Text Classification](https://arxiv.org/pdf/1908.02810.pdf). In Proceedings of the First Workshop on Gender Bias in Natural Language Processing (pp. 69-75).\n",
        "  \n",
        "  - Romanov, A., De-Arteaga, M., Wallach, H., Chayes, J., Borgs, C., Chouldechova, A., ... & Kalai, A. (2019, June). [What's in a Name? Reducing Bias in Bios without Access to Protected Attributes](https://arxiv.org/pdf/1904.05233.pdf). In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4187-4195).\n",
        "\n",
        "\n",
        "- **Other**\n",
        "  \n",
        "  - Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., & Chang, K. W. (2019, June). [Gender Bias in Contextualized Word Embeddings](https://arxiv.org/pdf/1904.03310.pdf). In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 629-634). [slides](https://jyzhao.net/files/naacl19.pdf)\n",
        "\n",
        "  - Zhou, P., Shi, W., Zhao, J., Huang, K. H., Chen, M., & Chang, K. W. [Analyzing and Mitigating Gender Bias in Languages with Grammatical Gender and Bilingual Word Embeddings](https://aiforsocialgood.github.io/icml2019/accepted/track1/pdfs/47_aisg_icml2019.pdf). ICML 2019 - AI for Social Good. [Poster](https://aiforsocialgood.github.io/icml2019/accepted/track1/posters/47_aisg_icml2019.pdf)\n",
        "\n",
        "- Zhao, J., Mukherjee, S., Hosseini, S., Chang, K. W., & Awadallah, A. [Gender Bias in Multilingual Embeddings](https://www.researchgate.net/profile/Subhabrata_Mukherjee/publication/340660062_Gender_Bias_in_Multilingual_Embeddings/links/5e97428692851c2f52a6200a/Gender-Bias-in-Multilingual-Embeddings.pdf).\n",
        "\n",
        "\n",
        "##### Complete example of using `responsibly` with Word2Vec, GloVe and fastText: http://docs.responsibly.ai/notebooks/demo-gender-bias-words-embedding.html\n",
        "\n",
        "\n",
        "## Bias in NLP\n",
        "\n",
        "Around dozen of papers on this field until 2019, but nowdays plenty of work is done. Two venues from back then:\n",
        "- [1st ACL Workshop on Gender Bias for Natural Language Processing](https://genderbiasnlp.talp.cat/)\n",
        "- [NAACL 2019](https://naacl2019.org/)\n"
      ]
    }
  ]
}